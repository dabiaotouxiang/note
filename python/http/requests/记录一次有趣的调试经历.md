最近在做爬虫验证的时候，遇到一个很有意思的问题，下面是调试的经过

1. 该验证，是先通过一系列的浏览器参数采集，获取足够的参数，生成一个cookie，该cookie会随着参数的更新而更新，其中一个比较重要的更新标志是cookie的结尾，最开始是`-1~-1~-1`，后面是`-1~||xxxxx||~-1`，然后浏览器端的js文件会根据`||xxxx||`对cookie进行一些计算，然后将结果返回，然后cookie的值就又会变成`-1~||-1||~-1`，然后这代表该浏览器已经在服务器注册完毕，通过初步检测。
2. 问题，昨天进行测试的时候，突然发现cookie一直是`-1~-1~-1`，一直没有变动，打开chrome浏览器调试工具，发现cookie的变化过程没有改变，所以以为是收集的参数的格式有变动，经过检查，发现多了两个参数，添加之后测试，发现还是没有变动，这时候有点心态崩了。
3. 将浏览器的请求参数拷贝下来，用requests.session进行请求，同样的参数，依然不行
4. 怀疑是一些被隐藏起来的请求参数的问题，于是用下面的代码进行requests请求报文的打印
```
pip install requests_toolbelt
from requests_toolbelt.utils.dump import dump_all
resp = self.session.post(url, headers=headers, json=data, timeout=(30, 30))
data = dump_all(resp)
print(data.decode('utf-8'))
```
打印出来之后，然后通过fiddler进行浏览器请求的抓包，查看报文的比对。

发现了resp的headers里面又一些不一样，同时，还发现，resp的值应该是一个代表接受成功的json对象，大小应该是不变的，但是requests的返回值长度是22，fiddler的抓取的chrome的返回值长度是17。
5. 这时候我又将发送到服务器的浏览器的sensor的值检查了遍，确认没有任何问题。这时候真是百思不得其解。然后，在fiddler没有关闭的情况下，我继续用requests进行请求，想抓requests的包。这时候，奇迹发生了，requests成功的请求到了正确的cookie
6. 关闭fiddler进行测试，又不成功了
7. 分析了一下fiddler的https抓包原理，初步怀疑是tls的问题，于是去网上找了requests指定tls1.2（fiddler的报文里，指明了请求为tls1.2），找到了如下的代码，拷贝进我的代码里，测试，成功
```
import requests
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util import ssl_
# 可以用wireshark或者fiddler抓包确定可选择的密钥
CIPHERS = (
    'AES128-GCM-SHA256:ECDHE-RSA-AES128-SHA256:AES256-SHA'
)


class TlsAdapter(HTTPAdapter):

    def __init__(self, ssl_options=0, **kwargs):
        self.ssl_options = ssl_options
        super(TlsAdapter, self).__init__(**kwargs)

    def init_poolmanager(self, *pool_args, **pool_kwargs):
        # 通过选定的加密方法对请求进行加密
        ctx = ssl_.create_urllib3_context(ciphers=CIPHERS, cert_reqs=ssl.CERT_REQUIRED, options=self.ssl_options)
        self.poolmanager = PoolManager(*pool_args,
                                       ssl_context=ctx,
                                       **pool_kwargs)


sess = requests.session()
# 这里的代码表明是专门组织tls1.3，可以用ssl.PROTOCOL_TLSv1_2专门指定tls1.2，其他的选项可以查资料确定含义
adp = TlsAdapter(ssl.OP_NO_TLSv1_3 | ssl.OP_NO_TLSv1_3)
sess.mount("https://", adapter)
```

8. 这时候已经半夜三点多了，还剩下加上proxy的测试，没在意，就去睡觉了
9. 第二天早上醒了之后，加上proxy进行测试，结果失败
10. wireshark抓包，抓取请求报文，server和client的hello信息，没发现加不加代理有什么不同。
11. 没办法，没有银弹，拷贝来的代码不经过修改，一般是达不到要求的。于是就查了查资料，确定了上面的代码的意思，然后把`ssl.OP_NO_TLSv1_3`修改成了`ssl.PROTOCOL_TLSv1_2`，同时把wireshark里面报文中chrome发送的可选的加密方式的前三个填上了，结果依然是不加代理可以，加代理不可以。
12. 没办法了，检查wireshark抓的包，发现不加代理的握手次数，好像比加了代理的握手次数少几次(当然这可能是我的错觉)
13. 于是开始单步调试，终于在调试到requests的源码里，adapters的send方法，发现了建立链接的代码
```
try:
    conn = self.get_connection(request.url, proxies)
except LocationValueError as e:
    raise InvalidURL(e, request=request)
```
14. 点进去，发现了问题，加不加代理是不同的处理逻辑，不加代理，就直接用poolmanager，加上代理就用proxy_manager，拷贝来的代码只处理了poolmanager
```
def get_connection(self, url, proxies=None):
    """Returns a urllib3 connection for the given URL. This should not be
    called from user code, and is only exposed for use when subclassing the
    :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

    :param url: The URL to connect to.
    :param proxies: (optional) A Requests-style dictionary of proxies used on this request.
    :rtype: urllib3.ConnectionPool
    """
    proxy = select_proxy(url, proxies)

    if proxy:
        proxy = prepend_scheme_if_needed(proxy, 'http')
        proxy_url = parse_url(proxy)
        if not proxy_url.host:
            raise InvalidProxyURL("Please check proxy URL. It is malformed"
                                  " and could be missing the host.")
        proxy_manager = self.proxy_manager_for(proxy)
        conn = proxy_manager.connection_from_url(url)
    else:
        # Only scheme should be lower case
        parsed = urlparse(url)
        url = parsed.geturl()
        conn = self.poolmanager.connection_from_url(url)
```
15. 修改拷贝来的代码，修改过程中配合wireshark发现，即使不设置tls版本，也可以
```
CIPHERS = (
    'AES128-GCM-SHA256:AES256-GCM-SHA384:CHACHA20-POLY1305-SHA256'
    # 'AES256-GCM'
)

try:
    from urllib3.contrib.socks import SOCKSProxyManager
except ImportError:
    def SOCKSProxyManager(*args, **kwargs):
        raise InvalidSchema("Missing dependencies for SOCKS support.")


def proxy_from_url(url, **kw):
    return ProxyManager(proxy_url=url, **kw)


def get_auth_from_url(url):
    """Given a url with authentication components, extract them into a tuple of
    username,password.

    :rtype: (str,str)
    """
    parsed = urlparse(url)

    try:
        auth = (unquote(parsed.username), unquote(parsed.password))
    except (AttributeError, TypeError):
        auth = ('', '')

    return auth


class TlsAdapter(HTTPAdapter):

    def __init__(self, **kwargs):
        super(TlsAdapter, self).__init__(**kwargs)

    def init_poolmanager(self, *pool_args, **pool_kwargs):
        ctx = ssl_.create_urllib3_context(ciphers=CIPHERS)
        self.poolmanager = PoolManager(*pool_args,
                                       ssl_context=ctx,
                                       **pool_kwargs)

    def proxy_manager_for(self, proxy, **proxy_kwargs):
        # 基本是复制的源码
        if proxy in self.proxy_manager:
            manager = self.proxy_manager[proxy]
        elif proxy.lower().startswith('socks'):
            username, password = get_auth_from_url(proxy)
            manager = self.proxy_manager[proxy] = SOCKSProxyManager(
                proxy,
                username=username,
                password=password,
                num_pools=self._pool_connections,
                maxsize=self._pool_maxsize,
                block=self._pool_block,
                **proxy_kwargs
            )
        else:
        # 在这做了修改，加上了ctx，因为源码中ProxyManager继承于PoolManager，所以直接和init_poolmanager一样的添加方法就行了
            ctx = ssl_.create_urllib3_context(ciphers=CIPHERS)
            proxy_headers = self.proxy_headers(proxy)
            manager = self.proxy_manager[proxy] = proxy_from_url(
                proxy,
                proxy_headers=proxy_headers,
                num_pools=self._pool_connections,
                maxsize=self._pool_maxsize,
                block=self._pool_block,
                ssl_context=ctx,
                **proxy_kwargs)

        return manager
```
16. 测试，成功
17. 改进一下代码，毕竟直接拷贝源码比较丑
```
from urllib3.util import ssl_
from requests.adapters import HTTPAdapter
CIPHERS = (
    'AES128-GCM-SHA256:AES256-GCM-SHA384:CHACHA20-POLY1305-SHA256'
    # 'AES256-GCM'
)


class TlsAdapter(HTTPAdapter):

    def __init__(self, **kwargs):
        super(TlsAdapter, self).__init__(**kwargs)

    def init_poolmanager(self, *pool_args, **pool_kwargs):
        ctx = ssl_.create_urllib3_context(ciphers=CIPHERS)
        return super().init_poolmanager(*pool_args, ssl_context=ctx, **pool_kwargs)

    def proxy_manager_for(self, proxy, **proxy_kwargs):

        ctx = ssl_.create_urllib3_context(ciphers=CIPHERS)

        return super().proxy_manager_for(proxy, ssl_context=ctx, **proxy_kwargs)
session = requests.session()
session.proxies = {"http": "http://" + proxy, "https": "https://" + proxy}
adp = TlsAdapter()
session.mount("https://", adp)
```

18. 后来经过查资料，发现应该是requests的tls指纹被设置为特征指纹了。经过代码中自己设置CIPHERS在一定程度上改变了requests的tls的指纹特征。所以就通过了。但是和chrome的tls的指纹依旧相差比较大